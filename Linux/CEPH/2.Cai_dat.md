. Các bước cài đặt
3.1. Thiết lập hostname, IP
3.1.1. Thiết lập hostname, IP cho node CEPH1

Login với tài khoản root và thực hiện các lệnh dưới.

yum update -y

Cài đặt các gói phần mềm bổ trợ

yum install epel-release -y
yum update -y
yum install wget byobu curl git byobu python-setuptools python-virtualenv -y

Thiết lập hostname cho CEPH1

hostnamectl set-hostname ceph1

Thiết lập IP Add, trong hướng dẫn này, VLAN 192.168.98.0/24 sẽ ra internet để tải các gói cài đặt.


echo "Setup IP  eth0"
nmcli con modify eth0 ipv4.addresses 192.168.98.85/24
nmcli con modify eth0 ipv4.gateway 192.168.98.1
nmcli con modify eth0 ipv4.dns 8.8.8.8
nmcli con modify eth0 ipv4.method manual
nmcli con mod eth0 connection.autoconnect yes

echo "Setup IP  eth1"
nmcli con modify eth1 ipv4.addresses 192.168.62.85/24
nmcli con modify eth1 ipv4.method manual
nmcli con mod eth1 connection.autoconnect yes

echo "Setup IP  eth2"
nmcli con modify eth2 ipv4.addresses 192.168.63.85/24
nmcli con modify eth2 ipv4.method manual
nmcli con mod eth2 connection.autoconnect yes

Cấu hình chế độ firewall để tiện trong môi trường lab. Trong môi trường production cần bật firewall hoặc iptables hoặc có biện pháp xử lý khác tương ứng để đảm bảo các vấn đề về an toàn.

sudo systemctl disable firewalld
sudo systemctl stop firewalld
sudo systemctl disable NetworkManager
sudo systemctl stop NetworkManager
sudo systemctl enable network
sudo systemctl start network

sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config

echo "net.ipv6.conf.all.disable_ipv6 = 1" >> /etc/sysctl.conf

Khai báo file /etc/hosts. Việc này rất quan trọng vì CEPH sẽ sử dụng hostname trong các bước tới để cấu hình và kết nối khi thực hiện.

cat << EOF > /etc/hosts
127.0.0.1 `hostname` localhost
192.168.62.84 client1
192.168.62.85 ceph1
192.168.62.86 ceph2
192.168.62.87 ceph3

192.168.98.84 client1
192.168.98.85 ceph1
192.168.98.86 ceph2
192.168.98.87 ceph3
EOF

Cài đặt NTP, trong hướng dẫn này sử dụng chronyd thay cho ntpd. Việc đồng bộ thời gian cũng là quan trọng khi triển khai CEPH. Hãy đảm bảo timezone và thời gian được đồng bộ để đúng với hệ thống của bạn.

yum install -y chronyd

systemctl enable chronyd.service
systemctl start chronyd.service
systemctl restart chronyd.service
chronyc sources

Lưu ý: Nếu bạn có hệ thống NTP nội bộ, hãy cấu hình nó hoặc tham khảo ở đây: https://news.cloud365.vn/?s=ntp

Khởi động lại node CEPH1 và chuyển sang CEPH2 thực hiện tiếp.

init 6

3.1.2. Thiết lập hostname, IP cho node CEPH2

Login với tài khoản root và thực hiện các lệnh dưới.

Cập nhật các gói phần mềm.

yum update -y

Cài đặt các gói phần mềm bổ trợ

yum install epel-release -y
yum update -y
yum install wget byobu curl git byobu python-setuptools python-virtualenv -y

Thiết lập hostname cho CEPH2

hostnamectl set-hostname ceph2

Thiết lập IP Add, trong hướng dẫn này, VLAN 192.168.98.0/24 sẽ ra internet để tải các gói cài đặt.

echo "Setup IP  eth0"
nmcli con modify eth0 ipv4.addresses 192.168.98.86/24
nmcli con modify eth0 ipv4.gateway 192.168.98.1
nmcli con modify eth0 ipv4.dns 8.8.8.8
nmcli con modify eth0 ipv4.method manual
nmcli con mod eth0 connection.autoconnect yes

echo "Setup IP  eth1"
nmcli con modify eth1 ipv4.addresses 192.168.62.86/24
nmcli con modify eth1 ipv4.method manual
nmcli con mod eth1 connection.autoconnect yes

echo "Setup IP  eth2"
nmcli con modify eth2 ipv4.addresses 192.168.63.86/24
nmcli con modify eth2 ipv4.method manual
nmcli con mod eth2 connection.autoconnect yes

Cấu hình chế độ firewall để tiện trong môi trường lab. Trong môi trường production cần bật firewall hoặc iptables hoặc có biện pháp xử lý khác tương ứng để đảm bảo các vấn đề về an toàn.

sudo systemctl disable firewalld
sudo systemctl stop firewalld
sudo systemctl disable NetworkManager
sudo systemctl stop NetworkManager
sudo systemctl enable network
sudo systemctl start network

sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config

echo "net.ipv6.conf.all.disable_ipv6 = 1" >> /etc/sysctl.conf

Khai báo file /etc/hosts. Việc này rất quan trọng vì CEPH sẽ sử dụng hostname trong các bước tới để cấu hình và kết nối khi thực hiện.

cat << EOF > /etc/hosts
127.0.0.1 `hostname` localhost
192.168.62.84 client1
192.168.62.85 ceph1
192.168.62.86 ceph2
192.168.62.87 ceph3

192.168.98.84 client1
192.168.98.85 ceph1
192.168.98.86 ceph2
192.168.98.87 ceph3
EOF

Cài đặt NTP, trong hướng dẫn này sử dụng chronyd thay cho ntpd. Việc đồng bộ thời gian cũng là quan trọng khi triển khai CEPH. Hãy đảm bảo timezone và thời gian được đồng bộ để đúng với hệ thống của bạn.

yum install -y chronyd

systemctl enable chronyd.service
systemctl start chronyd.service
systemctl restart chronyd.service
chronyc sources

Lưu ý: Nếu bạn có hệ thống NTP nội bộ, hãy cấu hình nó hoặc tham khảo ở đây: https://news.cloud365.vn/?s=ntp

Khởi động lại node CEPH2 và chuyển sang CEPH3 thực hiện.

init 6

3.1.3. Thiết lập hostname, IP cho node CEPH3

Login với tài khoản root và thực hiện các lệnh dưới.

Cập nhật các gói phần mềm.

yum update -y

Cài đặt các gói bổ trợ

yum install epel-release -y
yum update -y
yum install wget byobu curl git byobu python-setuptools python-virtualenv -y

Thiết lập hostname cho CEPH3

hostnamectl set-hostname ceph3

Thiết lập IP Add, trong hướng dẫn này, VLAN 192.168.98.0/24 sẽ ra internet để tải các gói cài đặt.

echo "Setup IP  eth0"
nmcli con modify eth0 ipv4.addresses 192.168.98.87/24
nmcli con modify eth0 ipv4.gateway 192.168.98.1
nmcli con modify eth0 ipv4.dns 8.8.8.8
nmcli con modify eth0 ipv4.method manual
nmcli con mod eth0 connection.autoconnect yes

echo "Setup IP  eth1"
nmcli con modify eth1 ipv4.addresses 192.168.62.87/24
nmcli con modify eth1 ipv4.method manual
nmcli con mod eth1 connection.autoconnect yes

echo "Setup IP  eth2"
nmcli con modify eth2 ipv4.addresses 192.168.63.87/24
nmcli con modify eth2 ipv4.method manual
nmcli con mod eth2 connection.autoconnect yes

Cấu hình chế độ firewall để tiện trong môi trường lab. Trong môi trường production cần bật firewall hoặc iptables hoặc có biện pháp xử lý khác tương ứng để đảm bảo các vấn đề về an toàn.

sudo systemctl disable firewalld
sudo systemctl stop firewalld
sudo systemctl disable NetworkManager
sudo systemctl stop NetworkManager
sudo systemctl enable network
sudo systemctl start network

sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/sysconfig/selinux
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config

echo "net.ipv6.conf.all.disable_ipv6 = 1" >> /etc/sysctl.conf

Khai báo file /etc/hosts. Việc này rất quan trọng vì CEPH sẽ sử dụng hostname trong các bước tới để cấu hình và kết nối khi thực hiện.

cat << EOF > /etc/hosts
127.0.0.1 `hostname` localhost
192.168.62.84 client1
192.168.62.85 ceph1
192.168.62.86 ceph2
192.168.62.87 ceph3

192.168.98.84 client1
192.168.98.85 ceph1
192.168.98.86 ceph2
192.168.98.87 ceph3
EOF

Cài đặt NTP, trong hướng dẫn này sử dụng chronyd thay cho ntpd. Việc đồng bộ thời gian cũng là quan trọng khi triển khai CEPH. Hãy đảm bảo timezone và thời gian được đồng bộ để đúng với hệ thống của bạn.

yum install -y chronyd

systemctl enable chronyd.service
systemctl start chronyd.service
systemctl restart chronyd.service
chronyc sources

Lưu ý: Nếu bạn có hệ thống NTP nội bộ, hãy cấu hình nó hoặc tham khảo ở đây: https://news.cloud365.vn/?s=ntp

Khởi động lại node CEPH2 và chuyển sang CEPH1 thực hiện các cấu hình tiếp theo. Login với các IP đã đặt cho CEPH trước đó.
3.2. Tạo user cài đặt CEPH và khai báo repos
3.2.1. Tạo user để cài đặt CEPH trên cả 02 node CEPH1, CEPH2, CEPH3.

Bước này được thực hiện trên cả 03 node CPEH.

Tạo user là cephuser với mật khẩu là matkhau2019@

useradd cephuser; echo 'matkhau2019@' | passwd cephuser --stdin

Cấp quyền sudo cho user cephuser

echo "cephuser ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/cephuser
chmod 0440 /etc/sudoers.d/cephuser

3.2.2. Khai báo repo cho ceph nautilus

Thực hiện hiện khai báo repo cho ceph nautilus trên tất cả các node CEPH.

cat <<EOF> /etc/yum.repos.d/ceph.repo
[ceph]
name=Ceph packages for $basearch
baseurl=https://download.ceph.com/rpm-nautilus/el7/x86_64/
enabled=1
priority=2
gpgcheck=1
gpgkey=https://download.ceph.com/keys/release.asc

[ceph-noarch]
name=Ceph noarch packages
baseurl=https://download.ceph.com/rpm-nautilus/el7/noarch
enabled=1
priority=2
gpgcheck=1
gpgkey=https://download.ceph.com/keys/release.asc

[ceph-source]
name=Ceph source packages
baseurl=https://download.ceph.com/rpm-nautilus/el7/SRPMS
enabled=0
priority=2
gpgcheck=1
gpgkey=https://download.ceph.com/keys/release.asc
EOF

Thực hiện update sau khi khai bao repo

yum update -y

3.3. Cài đặt ceph-deploy và cấu hình

Theo docs của CEPH thì ta có thể cài đặt CEPH theo 03 cách, bao gồm: CEPH manual, ceph-deploy và ceph-ansible. Trong hướng dẫn này sẽ sử dụng ceph-deploy, một công cụ để triển khai CEPH.
3.3.1 Cài đặt ceph-deploy.

Thực hiện việc cài đặt này trên node CEPH1

Trong một số mô hình, node cài đặt ceph-deploy được gọi là node admin. Trong hướng dẫn này, cloud365 sẽ sử dụng node ceph1 chính là node ceph admin. Thực hiện việc này bằng tài khoản root.

Lưu ý: trong hướng dẫn này chỉ cần đứng trên ceph1 thực hiện, một số thao tác trên node CEPH2, CEPH3 sẽ thực hiện từ xa ngay trên CEPH1.

sudo yum install -y epel-release
sudo yum install -y ceph-deploy

Chuyển sang tài khoản cephuser

su - cephuser

Tạo ssh key, sau đó copy sang các node còn lại, nhập mật khẩu của user cephuser ở trên khi được hỏi tại màn hình. Lưu ý không dùng sudo với lệnh ssh-keygen

ssh-keygen

Nhấn Enter để mặc định các tham số, bước này sẽ sinh ra private key và public key cho user cephuser. Sau đó tiến hành các lệnh dưới để copy public key sang các node.

Nhập mật khẩu của user cephuser tạo ở các node trước đó trong bước trên.

ssh-copy-id cephuser@ceph1

ssh-copy-id cephuser@ceph2

ssh-copy-id cephuser@ceph3

Tạo thư mục chứa các file cấu hình khi cài đặt CEPH

cd ~
mkdir my-cluster
cd my-cluster

Khai báo các node ceph trong cluser.

ceph-deploy new ceph1 ceph2 ceph3

Lệnh trên sẽ sinh ra các file cấu hình trong thư mục hiện tại, kiểm tra bằng lệnh ls – alh

cephuser@ceph1:~/my-cluster$ ls -alh
total 188K
drwxrwxr-x 2 cephuser cephuser   75 Sep  6 23:14 .
drwx------ 5 cephuser cephuser  151 Sep  6 23:14 ..
-rw-rw-r-- 1 cephuser cephuser  418 Sep  6 23:15 ceph.conf
-rw-rw-r-- 1 cephuser cephuser 177K Sep  6 23:19 ceph-deploy-ceph.log
-rw------- 1 cephuser cephuser   73 Sep  6 23:14 ceph.mon.keyring

Khai báo thêm các tùy chọn cho việc triển khai, vận hành CEPH vào file ceph.conf này trước khi cài đặt các gói cần thiết cho ceph trên các node. Lưu ý các tham số về network.

Ta sẽ dụng vlan 192.168.62.0/24 cho đường truy cập của các client (Hay gọi là ceph public. Vlan 192.168.63.0/24 cho đường replicate dữ liệu, các dữ liệu sẽ được sao chép & nhân bản qua vlan này.

echo "public network = 192.168.62.0/24" >> ceph.conf
echo "cluster network = 192.168.63.0/24" >> ceph.conf
echo "osd objectstore = bluestore"  >> ceph.conf
echo "mon_allow_pool_delete = true"  >> ceph.conf
echo "osd pool default size = 3"  >> ceph.conf
echo "osd pool default min size = 1"  >> ceph.conf

Bắt đầu cài đặt phiên bản CEPH Nautilus lên các node ceph1, ceph2, ceph3. Lệnh dưới sẽ cài đặt lần lượt lên các node.

ceph-deploy install --release nautilus ceph1 ceph2 ceph3

Kết quả của lệnh trên sẽ hiển thị như bên dưới, trong đó có phiên bản của ceph được cài trên các node.

.......

[2019-09-06 23:16:46,139][ceph1][INFO  ] Running command: sudo ceph --version
[2019-09-06 23:16:46,262][ceph1][DEBUG ] ceph version 14.2.3 (0f776cf838a1ae3130b2b73dc26be9c95c6ccc39) nautilus (stable)
.......

[2019-09-06 23:18:18,771][ceph2][INFO  ] Running command: sudo ceph --version
[2019-09-06 23:18:18,944][ceph2][DEBUG ] ceph version 14.2.3 (0f776cf838a1ae3130b2b73dc26be9c95c6ccc39) nautilus (stable)
.......

[2019-09-06 23:19:46,856][ceph3][INFO  ] Running command: sudo ceph --version
[2019-09-06 23:19:47,028][ceph3][DEBUG ] ceph version 14.2.3 (0f776cf838a1ae3130b2b73dc26be9c95c6ccc39) nautilus (stable)

Thiết lập thành phần MON cho CEPH. Trong hướng dẫn này khai báo 03 node đều có thành phần MON của CEPH.

ceph-deploy mon create-initial

Kết quả sinh ra các file trong thư mục hiện tại

cephuser@ceph1:~/my-cluster$ ls -alh
total 348K
drwxrwxr-x 2 cephuser cephuser  244 Sep  6 23:26 .
drwx------ 5 cephuser cephuser  151 Sep  6 23:14 ..
-rw------- 1 cephuser cephuser  113 Sep  6 23:26 ceph.bootstrap-mds.keyring
-rw------- 1 cephuser cephuser  113 Sep  6 23:26 ceph.bootstrap-mgr.keyring
-rw------- 1 cephuser cephuser  113 Sep  6 23:26 ceph.bootstrap-osd.keyring
-rw------- 1 cephuser cephuser  113 Sep  6 23:26 ceph.bootstrap-rgw.keyring
-rw------- 1 cephuser cephuser  151 Sep  6 23:26 ceph.client.admin.keyring
-rw-rw-r-- 1 cephuser cephuser  418 Sep  6 23:15 ceph.conf
-rw-rw-r-- 1 cephuser cephuser 214K Sep  6 23:26 ceph-deploy-ceph.log
-rw------- 1 cephuser cephuser   73 Sep  6 23:14 ceph.mon.keyring

Thực hiện copy file ceph.client.admin.keyring sang các node trong cụm ceph cluster. File này sẽ được copy vào thư mục /etc/ceph/ trên các node.

ceph-deploy admin ceph1 ceph2 ceph3

Kết quả của lệnh trên như sau

cephuser@ceph1:~/my-cluster$ ceph-deploy admin ceph1 ceph2 ceph3
[ceph_deploy.conf][DEBUG ] found configuration file at: /home/cephuser/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (2.0.1): /bin/ceph-deploy admin ceph1 ceph2 ceph3
[ceph_deploy.cli][INFO  ] ceph-deploy options:
[ceph_deploy.cli][INFO  ]  username                      : None
[ceph_deploy.cli][INFO  ]  verbose                       : False
[ceph_deploy.cli][INFO  ]  overwrite_conf                : False
[ceph_deploy.cli][INFO  ]  quiet                         : False
[ceph_deploy.cli][INFO  ]  cd_conf                       : <ceph_deploy.conf.cephdeploy.Conf instance at 0x7f3c031d6248>
[ceph_deploy.cli][INFO  ]  cluster                       : ceph
[ceph_deploy.cli][INFO  ]  client                        : ['ceph1', 'ceph2', 'ceph3']
[ceph_deploy.cli][INFO  ]  func                          : <function admin at 0x7f3c03a77230>
[ceph_deploy.cli][INFO  ]  ceph_conf                     : None
[ceph_deploy.cli][INFO  ]  default_release               : False
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph1
[ceph1][DEBUG ] connection detected need for sudo
[ceph1][DEBUG ] connected to host: ceph1
[ceph1][DEBUG ] detect platform information from remote host
[ceph1][DEBUG ] detect machine type
[ceph1][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph2
[ceph2][DEBUG ] connection detected need for sudo
[ceph2][DEBUG ] connected to host: ceph2
[ceph2][DEBUG ] detect platform information from remote host
[ceph2][DEBUG ] detect machine type
[ceph2][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph3
[ceph3][DEBUG ] connection detected need for sudo
[ceph3][DEBUG ] connected to host: ceph3
[ceph3][DEBUG ] detect platform information from remote host
[ceph3][DEBUG ] detect machine type
[ceph3][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf

Đứng trên node ceph1 phân quyền cho file /etc/ceph/ceph.client.admin.keyring cho cả 03 node.

ssh cephuser@ceph1 'sudo chmod +r /etc/ceph/ceph.client.admin.keyring'
ssh cephuser@ceph2 'sudo chmod +r /etc/ceph/ceph.client.admin.keyring'
ssh cephuser@ceph3 'sudo chmod +r /etc/ceph/ceph.client.admin.keyring'

3.3.2 Khai báo các OSD cho node CEPH.

Đứng trên node ceph1 và thực hiện khai báo các OSD disk. Bước này sẽ thực hiện format các disk trên cả 3 node và join chúng vào làm các OSD (Thành phần chứa dữ liệu của CEPH).

ceph-deploy osd create --data /dev/vdb ceph1

ceph-deploy osd create --data /dev/vdc ceph1

ceph-deploy osd create --data /dev/vdd ceph1


ceph-deploy osd create --data /dev/vdb ceph2

ceph-deploy osd create --data /dev/vdc ceph2

ceph-deploy osd create --data /dev/vdd ceph2


ceph-deploy osd create --data /dev/vdb ceph3

ceph-deploy osd create --data /dev/vdc ceph3

ceph-deploy osd create --data /dev/vdd ceph3

Tới đây các bước cơ bản cấu hình ceph cluser đã hoàn tất. Kiểm tra trạng thái của cụm cluster ceph bằng lệnh ceph -s

Kết quả lệnh ceph -s sẽ như sau

cephuser@ceph1:~/my-cluster$ ceph -s
  cluster:
    id:     9691b2b5-a858-45b7-a239-4de3e1ff69c6
    health: HEALTH_WARN
            no active mgr

  services:
    mon: 3 daemons, quorum ceph1,ceph2,ceph3 (age 4m)
    mgr: no daemons active
    osd: 8 osds: 8 up (since 6s), 8 in (since 6s)

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:

Ta thấy trạng thái sẽ là HEALTH_WARN, lý do là vì ceph-mgr chưa được enable. Tiếp theo ta sẽ xử lý để kích hoạt ceph-mgr

3.3.3. Cấu hình manager và dashboad cho ceph cluster

Ceph-dashboard là một thành phần thuộc ceph-mgr. Trong bản Nautilus thì thành phần dashboard được cả tiến khá lớn. Cung cấp nhiều quyền hạn thao tác với CEPH hơn các bản trước đó (thành phần này được đóng góp chính bởi team SUSE).

Thực hiện trên node ceph1 việc cài đặt này.

Trong bản ceph nautilus 14.2.3 (tính đến ngày 06.09.2019), khi cài ceph dashboard theo các cách cũ gặp một vài vấn đề, cách xử lý như sau.

Cài thêm các gói bổ trợ trước khi cài

sudo yum install -y python-jwt python-routes

Tải ceph-dashboad ceph-mgr-dashboard-14.2.3-0.el7.noarch.rpm và ceph-grafana-dashboards-14.2.4-0.el7.noarch.rpm. Lưu ý nên đúng phiên bản với ceph ở trên.

sudo rpm -Uvh http://download.ceph.com/rpm-nautilus/el7/noarch/ceph-grafana-dashboards-14.2.4-0.el7.noarch.rpm

sudo rpm -Uvh http://download.ceph.com/rpm-nautilus/el7/noarch/ceph-mgr-dashboard-14.2.4-0.el7.noarch.rpm

Thực hiện kích hoạt ceph-mgr và ceph-dashboard

ceph-deploy mgr create ceph1 ceph2 ceph3
 
yum install ceph-mgr-dashboard -y
ceph mgr module enable dashboard
ceph dashboard create-self-signed-cert
ceph mgr services


> Lưu ý cài đặt yum install ceph-mgr-dashboard -y trên cả ceph01 và ceph02 ceph3,